# Deep learning materials

This is a curated repository of materials for deep learning.

# Table of Contents

- [Courses](#courses)
- [Tutorials](#tutorials)
- [Curated lists](#curated-lists)
- [Papers](#papers)
- [Articles](#articles)
- [Books](#books)
- [Reports](#reports)
- [Models](#models)
- [Benchmarks](#benchmarks)
- [Datasets](#datasets)
- [Tools](#tools)
- [APIs](#apis)
- [Initiatives](#initiatives)
- [Lectures](#lectures)

## <a name='Courses'></a>Courses
- [Introduction to Deep Learning (MIT)](http://introtodeeplearning.com/)
- [Introduction to Deep Learning (Carnegie Mellon)](http://deeplearning.cs.cmu.edu/S24/index.html)
- [Deep Learning (Stanford)](https://cs230.stanford.edu/lecture/)
- [Mathematical Engineering of Deep Learning](https://deeplearningmath.org/)
- [Generative AI for beginners](https://microsoft.github.io/generative-ai-for-beginners/#/)
- Stanford Cheatsheets:
  - [Machine Learning](https://stanford.edu/~shervine/teaching/cs-229/)
  - [Deep Learning](https://stanford.edu/~shervine/teaching/cs-230/)
- Courses by Sebastian Raschka:
  - [Introduction to Deep Learning](https://sebastianraschka.com/blog/2021/dl-course.html)
  - [Introduction to Machine Learning](https://sebastianraschka.com/blog/2021/ml-course.html)
  - [Deep Learning Fundamentals](https://lightning.ai/courses/deep-learning-fundamentals/)
- [Prompt Engineering Guide](https://www.promptingguide.ai/)
- [Foundations of Deep Learning (University of Maryland)](http://www.cs.umd.edu/class/fall2020/cmsc828W/)
- [K-12 Prompt Engineering Guide](https://www.k12promptguide.com/home)

## <a name='Tutorials'></a>Tutorials
- [Build a Large Language Model (From Scratch)](https://github.com/rasbt/LLMs-from-scratch)
- [GPT in 60 lines of NumPy](https://jaykmody.com/blog/gpt-from-scratch/)
- [Implementing Mixtral with Neural Circuit Diagrams](https://github.com/vtabbott/Neural-Circuit-Diagrams/blob/main/mixtral.ipynb)
- [Hello Evo: From DNA generation to protein folding](https://colab.research.google.com/github/evo-design/evo/blob/main/scripts/hello_evo.ipynb)
- University of Amsterdam Deep Learning Tutorials [web](https://uvadlc-notebooks.readthedocs.io/en/latest/index.html) [Github](https://github.com/phlippe/uvadlc_notebooks)
- [Zero to LitGPT: Getting Started with Pretraining, Finetuning, and Using LLMs](https://github.com/Lightning-AI/litgpt/blob/main/tutorials/0_to_litgpt.md)
- Annotated Deep Learinng Research Papers Implementations [web](https://nn.labml.ai) [Github](https://github.com/labmlai/annotated_deep_learning_paper_implementations)
- [llamafile is the new best way to run a LLM on your own computer](https://simonwillison.net/2023/Nov/29/llamafile/)
- [Bash One-Liners for LLMs](https://justine.lol/oneliners/)

## <a name='Curated lists'></a>Curated lists
- [The Incredible PyTorch](https://github.com/ritchieng/the-incredible-pytorch/)
- [Statistics and machine learning: from undergraduate to research](https://github.com/dobriban/stat-ml-edu)
- [Awesome Graph-Related Large Language Models](https://github.com/XiaoxinHe/Awesome-Graph-LLM)
- [Deep Learning Drizzle](https://deep-learning-drizzle.github.io/)

## <a name='Papers'></a>Papers
- Role play with large language models (2023) [Nature.com](https://www.nature.com/articles/s41586-023-06647-8) [arXiv](https://arxiv.org/pdf/2305.16367.pdf)
- Attention is All You Need (2017) [arXiv v7 (2023)](https://arxiv.org/pdf/1706.03762.pdf)
- OLMo: Accelerating the Science of Language Models (2024) [arXiv](https://arxiv.org/pdf/2402.00838.pdf)
- Scaling Rectified Flow Transformers for High-Resolution Image Synthesis (2024) [blog](https://stability.ai/news/stable-diffusion-3-research-paper) [paper](https://stabilityai-public-packages.s3.us-west-2.amazonaws.com/Stable+Diffusion+3+Paper.pdf)
- Datasets for Large Language Models: A Comprehensive Survey (2024) [arXiv](https://arxiv.org/abs/2402.18041)
- Large Language Models(LLMs) on Tabular Data (2024) [arXiv](https://arxiv.org/abs/2402.17944)
- Chain-of-Thought Reasoning Without Prompting (2024) [arXiv](https://arxiv.org/abs/2402.10200)
- Holistic Evaluation of Language Models (2022) [arXiv](https://arxiv.org/abs/2211.09110)
- Understanding LLMs: A Comprehensive Overview from Training to Inference (2024) [arXiv](https://arxiv.org/abs/2401.02038)
- Large Language Models: A Survey (2024) [arXiv](https://arxiv.org/abs/2402.06196)
- Grandmaster-Level Chess Without Search (2024) [arXiv](https://arxiv.org/abs/2402.04494)
- GaLore: Memory-Efficient LLM Training by Gradient Low-Rank Projection (2024) [arXiv](https://arxiv.org/abs/2403.03507) [Github](https://github.com/jiaweizzhao/galore)
- MacGyver: Are Large Language Models Creative Problem Solvers? (2024) [arXiv](https://arxiv.org/abs/2311.09682) [Github](https://github.com/allenai/MacGyver)
- Revisiting Unreasonable Effectiveness of Data in Deep Learning Era (2017) [paper](https://openaccess.thecvf.com/content_ICCV_2017/papers/Sun_Revisiting_Unreasonable_Effectiveness_ICCV_2017_paper.pdf)
- Understanding Deep Learning Requires Rethinking Generalization (2017) [arXiv](https://arxiv.org/abs/1611.03530)
- Understanding Deep Learning (Still) Requires Rethinking Generalization (2021) [paper](https://dl.acm.org/doi/10.1145/3446776)
- Grokking: Generalization Beyond Overfitting on Small Algorithmic Datasets (2022) [arXiv](https://arxiv.org/abs/2201.02177)
- Deep Double Descent: Where Bigger Models and More Data Hurt (2019) [arXiv](https://arxiv.org/abs/1912.02292)
- Unifying Grokking and Double Descent (2023) [arXiv](https://arxiv.org/abs/2303.06173)
- Textbooks Are All You Need (2023) [arXiv](https://arxiv.org/abs/2306.11644)
- LoRa: Low-Rank Adaptation of Large Language Models (2021) [arXiv](https://arxiv.org/abs/2106.09685)
- Fundamental Components of Deep Learning: A category-theoretic approach (2023) [arXiv](https://arxiv.org/abs/2403.13001)
- Compression Represents Intelligence Linearly [arXiv](https://arxiv.org/abs/2404.09937) [HuggingFace](https://huggingface.co/papers/2404.09937)

## <a name='Articles'></a>Articles
- [RLHF: Reinforcement Learning from Human Feedback](https://huyenchip.com/2023/05/02/rlhf.html)
- [How do LLMs give truthful answers?](https://www.lesswrong.com/posts/ZKksgfTxuxKhDfk4m/how-do-llms-give-truthful-answers-a-discussion-of-llm-vs)
- [Models All the Way Down](https://knowingmachines.org/models-all-the-way)
- [Large language models can do jaw-dropping things. But nobody knows exactly why](https://www.technologyreview.com/2024/03/04/1089403/large-language-models-amazing-but-nobody-knows-why/)
- [How Chain-of-Thought Reasoning Helps Neural Networks Compute](https://www.quantamagazine.org/how-chain-of-thought-reasoning-helps-neural-networks-compute-20240321/)
- [Will Transformers Take Over Artificial Intelligence?](https://www.quantamagazine.org/will-transformers-take-over-artificial-intelligence-20220310/)
- [The Hidden Convex Optimization Landscape of Two-Layer ReLU Networks](https://iclr-blogposts.github.io/2024/blog/hidden-convex-relu/)
- [Building Diffusion Model's theory from ground up](https://iclr-blogposts.github.io/2024/blog/diffusion-theory-from-scratch/)
- [Generative AI exists because of the transformer](https://ig.ft.com/generative-ai/)

## <a name='Books'></a>Books
- [Understanding Deep Learning (2023)](https://udlbook.github.io/udlbook/)
- [Deep Learning](https://www.deeplearningbook.org/)
- [Dive into Deep Learning](https://d2l.ai/index.html)
- [Hands-On Machine Learning with R](https://bradleyboehmke.github.io/HOML/index.html)
- [Interpretable Machine Learing](https://christophm.github.io/interpretable-ml-book/)
- [Supervised Machine Learning for Science](https://ml-science-book.com/)
- [Machine Learning Engineering Open Book](https://github.com/stas00/ml-engineering)
- [Little Book of Deep Learning](https://fleuret.org/public/lbdl.pdf)

## <a name='Reports'></a>Reports
- [The State of Competitive Machine Learning (2023)](https://mlcontests.com/state-of-competitive-machine-learning-2023/?es_id=4476a44c3d#competitive-ml-landscape)

## <a name='Models'></a>Models
- Moondream - tiny vision language model [web](https://moondream.ai/) [Github](https://github.com/vikhyat/moondream)
- BioMistral - pretrained LLM models for biomedical domain [arXiv](https://arxiv.org/pdf/2402.10373.pdf) [HuggingFace](https://huggingface.co/BioMistral/BioMistral-7B)
- StructLM - Generalist Model for Structured Knowledge Grounding [web](https://tiger-ai-lab.github.io/StructLM/) [arXiv](https://arxiv.org/abs/2402.16671) [Github](https://github.com/TIGER-AI-Lab/StructLM) [HuggingFace](https://huggingface.co/datasets/TIGER-Lab/SKGInstruct)
- Gemma - Open Models Based on Gemini Research and Technology [HuggingFace](https://huggingface.co/collections/google/gemma-release-65d5efbccdbb8c4202ec078b) [paper](https://storage.googleapis.com/deepmind-media/gemma/gemma-report.pdf)
- Large World Model [web](https://largeworldmodel.github.io/) [Github](https://github.com/LargeWorldModel/LWM) [HuggingFace](https://huggingface.co/LargeWorldModel) [arXiv](https://arxiv.org/abs/2402.08268)
- ChemLLM: A Chemical Large Language Model [arXiv](https://arxiv.org/abs/2402.06852) [HuggingFace](https://huggingface.co/AI4Chem/ChemLLM-7B-Chat)
- BLOOM: A 176B-Parameter Open-Access Multilingual Language Model [arXiv](https://arxiv.org/abs/2211.05100) [HuggingFace](https://huggingface.co/bigscience/bloom) [web](https://bigscience.huggingface.co/blog/bloom)
- Llama 3 [web](https://ai.meta.com/blog/meta-llama-3/) [Github](https://github.com/meta-llama/llama3/tree/main)

## <a name='Benchmarks'></a>Benchmarks
- [Nicolas Carlini's benchmark of 100 tests for LLM's](https://nicholas.carlini.com/writing/2024/my-benchmark-for-large-language-models.html)

## <a name='Datasets'></a>Datasets
- Dolma: an Open Corpus of Three Trillion Tokens for Language Model Pretraining Research [arXiv](https://arxiv.org/pdf/2402.00159.pdf) [HuggingFace](https://huggingface.co/datasets/allenai/dolma)
- Internet Archive Public Domain English Books [HuggingFace](https://huggingface.co/datasets/storytracer/internet_archive_books_en)
- Multimodal ArXiv: A Dataset for Improving Scientific Comprehension of Large Vision-Language Models [web](https://mm-arxiv.github.io/) [arXiv](https://arxiv.org/abs/2403.00231)
- Cosmopedia - dataset of synthetic textbooks, blogposts, stories, posts and WikiHow articles [web](https://huggingface.co/blog/cosmopedia) [HuggingFace](https://huggingface.co/datasets/HuggingFaceTB/cosmopedia)
- FineWeb - 15T tokens of deduplicated and English texts from CommonCrawl [HuggingFace](https://huggingface.co/datasets/HuggingFaceFW/fineweb)

## <a name='Tools'></a>Tools
- Ollama - run Llama 2, Mistral and Gemma locally [web](https://ollama.com/) [Github](https://github.com/ollama/ollama)
- llama.cpp - standalone LLM inference in C/C++ for Llama models [Github](https://github.com/ggerganov/llama.cpp)
- gemma.cpp - standalone LLM inference in c/C++ for Gemma models [Github](https://github.com/google/gemma.cpp)
- DSPy - framework for programming foundational models [arXiV](https://arxiv.org/abs/2310.03714) [Github](https://github.com/stanfordnlp/dspy)
- LangChain - framework for buidling LLM applications [web](https://python.langchain.com/docs/get_started/introduction) [Github](https://python.langchain.com/docs/get_started/introduction)
- trl - Transformer Reinforcement Learning [Github](https://github.com/huggingface/trl)
- Transformers - State-of-the-art Machine Learning for PyTorch, TensorFlow, and JAX [Github](https://github.com/huggingface/transformers) [HuggingFace](https://huggingface.co/docs/transformers/index)
- Transformers.js - State-of-the-art Machine Learning for the web [HuggingFace](https://huggingface.co/docs/transformers.js/index)
- [Tokenizer Playground](https://huggingface.co/spaces/Xenova/the-tokenizer-playground)
- [Neural Networks Playground](https://playground.tensorflow.org/)
- MelloTTS - multi-lingual text-to-speech library [HuggingFace demo](https://huggingface.co/spaces/mrfakename/MeloTTS) [Github](https://github.com/myshell-ai/MeloTTS?tab=readme-ov-file)
- Finetune Mistral, Gemma, Llama 2-5x faster with 70% less memory! [Github](https://github.com/unslothai/unsloth)
- llm.c - LLM (GPT-2) training in simple, pure C/CUDA [Github](https://github.com/karpathy/llm.c/tree/master)
- LLM transparency tool [Github](https://github.com/facebookresearch/llm-transparency-tool)

## <a name='APIs'></a>APIs
- [GroqChat](https://groq.com/)
- [ChatGPT](https://chat.openai.com/)
- [Gemini](https://gemini.google.com/)
- [Claude](https://claude.ai/)

## <a name='Initiatives'></a>Initiatives
- Occiglot - research collective for open-source European LLMs [web](https://occiglot.github.io/occiglot/) [HuggingFace](https://huggingface.co/collections/occiglot/occiglot-eu5-7b-v01-65dbed502a6348b052695e01)
- Open Sora [Github](https://github.com/hpcaitech/Open-Sora)

## <a name='Lectures'></a>Lectures
- A little guide to building Large Language Models in 2024 [Youtube](https://www.youtube.com/watch?v=2-SPH9hIKT8&ab_channel=ThomWolf) [slides](https://docs.google.com/presentation/d/1IkzESdOwdmwvPxIELYJi8--K3EZ98_cL6c5ZcLKSyVg/edit#slide=id.p)
- Intro to LLMs by Andrey Karpathy [Youtube](https://www.youtube.com/watch?v=zjkBMFhNj_g&t=935s&ab_channel=AndrejKarpathy) [slides](https://drive.google.com/file/d/1pxx_ZI7O-Nwl7ZLNk5hI3WzAsTLwvNU7/view)
- The spelled-out intro to neural networks and backpropagation: building micrograd by Andrey Karpathy [Youtube](https://www.youtube.com/watch?v=VMj-3S1tku0)
- Attention in transformers, visually explained (Chapter 6, Deep Learning) [Youtube](https://www.youtube.com/watch?v=eMlx5fFNoYc)
- Intuition Behind Self-Attention Mechanism in Transformer Networks [Youtube](https://www.youtube.com/watch?v=g2BRIuln4uc&t=2s&ab_channel=Ark)

